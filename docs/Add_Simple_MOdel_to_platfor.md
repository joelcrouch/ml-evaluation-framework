# Onboarding a Simple Model: The Linear Time Series Example

## Overview

This guide demonstrates how to add a **simple** model to the ML Evaluation Platform - one that has the **same input/output shape** as the baseline model. We'll use the Linear model from the TensorFlow tutorial as our example.

**Why is this "simple"?** Because the Linear model uses the exact same window configuration as the Baseline:
- Input: 1 timestep × 19 features
- Output: 1 timestep × 1 feature (temperature)
- Window: `input_width=1, label_width=1, shift=1`

This means we can **reuse almost everything** from the baseline implementation with minimal changes.

---

## Understanding Why This Works

### The Baseline Model Architecture
```python
# From train_baseline_time_series.py
single_step_window = WindowGenerator(
    input_width=1,   # Look at current hour (1 timestep)
    label_width=1,   # Predict next hour (1 timestep)
    shift=1,         # Predict 1 hour into future
    label_columns=['T (degC)']  # Only predict temperature
)

baseline = Baseline(label_index=column_indices['T (degC)'])
# Prediction strategy: Return current temperature (assume no change)
```

**Input shape:** `(batch, 1, 19)` - One timestep with all 19 weather features  
**Output shape:** `(batch, 1, 1)` - One timestep with temperature prediction  

### The Linear Model Architecture
```python
# From TensorFlow tutorial
linear = tf.keras.Sequential([
    tf.keras.layers.Dense(units=1)  # Single dense layer
])

# Uses the SAME window
single_step_window = WindowGenerator(
    input_width=1,   # Same: Look at current hour
    label_width=1,   # Same: Predict next hour
    shift=1,         # Same: Predict 1 hour into future
    label_columns=['T (degC)']  # Same: Only temperature
)
```

**Input shape:** `(batch, 1, 19)` - Identical to baseline  
**Output shape:** `(batch, 1, 1)` - Identical to baseline  
**Prediction strategy:** Learn a weighted combination of all 19 features

### Key Insight: Same Shapes = Maximum Reuse

Because both models have identical input/output shapes, we can reuse:

✅ **KerasTimeSeriesModel** - Generic wrapper that loads any `.keras` file  
✅ **KerasTimeSeriesAdapter** - Works with any KerasTimeSeriesModel  
✅ **MeanSquaredErrorEvaluator** - Compares predictions vs ground truth  
✅ **Golden Dataset** - Same format (1 timestep in, 1 timestep out)  
✅ **Report Generator** - Same metrics (MSE, MAE, RMSE)  

The **only** things that change are:
- The model file name (`linear_model.keras`)
- The model_type identifier (`"time_series_linear"`)

---

## The Four-Stage Process

### Stage 1: Train & Save Model Assets

Create your training script (e.g., `train_linear_model.py`):
```python
# train_linear_time_series.py
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # Suppress TensorFlow logging

import tensorflow as tf
import numpy as np
import pandas as pd
import glob
import json     #  HMMM why did we add this?=> golden_dataset.json************************************************************************
import argparse # Added for command-line arguments**************************

MODEL_DIR = "models"
# Assuming 'processed_data.csv' is generated by the full tutorial setup
PROCESSED_DATA_PATH = "models/processed_data.csv" 

# Note: The WindowGenerator class is unchanged.
class WindowGenerator():
    def __init__(self, input_width, label_width, shift,
                 train_df, val_df, test_df,
                 label_columns=None):
        self.train_df = train_df
        self.val_df = val_df
        self.test_df = test_df

        self.label_columns = label_columns
        if label_columns is not None:
            self.label_columns_indices = {name: i for i, name in enumerate(label_columns)}
        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}

        self.input_width = input_width
        self.label_width = label_width
        self.shift = shift

        self.total_window_size = input_width + shift

        self.input_slice = slice(0, input_width)
        self.input_indices = np.arange(self.total_window_size)[self.input_slice]

        self.label_start = self.total_window_size - self.label_width
        self.labels_slice = slice(self.label_start, None)
        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]

    def split_window(self, features):
        inputs = features[:, self.input_slice, :]
        labels = features[:, self.labels_slice, :]
        if self.label_columns is not None:
            labels = tf.stack(
                [labels[:, :, self.column_indices[name]] for name in self.label_columns],
                axis=-1)
        inputs.set_shape([None, self.input_width, None])
        labels.set_shape([None, self.label_width, None])
        return inputs, labels

    def make_dataset(self, data):
        data = np.array(data, dtype=np.float32)
        ds = tf.keras.utils.timeseries_dataset_from_array(
            data=data,
            targets=None,
            sequence_length=self.total_window_size,
            sequence_stride=1,
            shuffle=True,
            batch_size=32,)
        ds = ds.map(self.split_window)
        return ds

    @property
    def train(self):
        return self.make_dataset(self.train_df)

    @property
    def val(self):
        return self.make_dataset(self.val_df)

    @property
    def test(self):
        return self.make_dataset(self.test_df)

def create_golden_dataset(test_df, window_config, num_samples=50, output_path='data/golden_dataset.json'):
    """
    Creates a golden dataset from the test set for evaluation.
    """
    input_width = window_config['input_width']
    label_width = window_config['label_width']
    shift = window_config['shift']
    label_column = window_config['label_column']
    
    golden_cases = []
    
    # --- UPDATED LOGIC ---
    # Calculate max possible start index
    max_start_idx = len(test_df) - (input_width + shift)
    
    # If num_samples is -1 or exceeds max possible, use all possible samples
    if num_samples == -1 or num_samples > max_start_idx + 1:
        print(f"Using all {max_start_idx + 1} possible test cases.")
        indices = np.arange(max_start_idx + 1)
    else:
        # Create evenly spaced samples across the test set
        print(f"Creating a sample of {num_samples} test cases.")
        indices = np.linspace(0, max_start_idx, num_samples, dtype=int)
    # --- END UPDATED LOGIC ---

    for i, start_idx in enumerate(indices):
        input_end = start_idx + input_width
        input_window = test_df.iloc[start_idx:input_end].values.tolist()
        
        label_start = start_idx + input_width + shift - 1
        label_end = label_start + label_width
        ground_truth = test_df.iloc[label_start:label_end][[label_column]].values.tolist()
        
        golden_case = {
            "case_id": i + 1,
            "input_data": {"window": input_window},
            "ground_truth": {"prediction": ground_truth},
            "metadata": {
                "start_index": int(start_idx),
                "input_width": input_width,
                "label_width": label_width,
                "shift": shift,
                "label_column": label_column
            }
        }
        golden_cases.append(golden_case)
    
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(golden_cases, f, indent=2)
    
    print(f"\n✅ Created {len(golden_cases)} golden test cases")
    print(f"   Saved to: {output_path}")
    
    return golden_cases


def main(args):
    """Main function to train the model and generate datasets."""
    if not os.path.exists(PROCESSED_DATA_PATH):
        print(f"❌ Processed data not found at {PROCESSED_DATA_PATH}.")
        exit(1)

    df = pd.read_csv(PROCESSED_DATA_PATH)
    print(f"Loaded processed data: {df.shape}")

    n = len(df)
    train_df = df[0:int(n*0.7)]
    val_df = df[int(n*0.7):int(n*0.9)]
    test_df = df[int(n*0.9):]

    print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

    num_features = len(df.columns)

    single_step_window = WindowGenerator(
        input_width=1, label_width=1, shift=1,
        train_df=train_df, val_df=val_df, test_df=test_df,
        label_columns=['T (degC)']
    )

    linear_model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=(single_step_window.input_width, num_features))
    ])

    linear_model.compile(
        loss=tf.keras.losses.MeanSquaredError(),
        metrics=[tf.keras.metrics.MeanAbsoluteError()]
    )

    MAX_EPOCHS = 20
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                       patience=2,
                                                       mode='min',
                                                       restore_best_weights=True)

    print("\nTraining linear model...")
    history = linear_model.fit(single_step_window.train, epochs=MAX_EPOCHS,
                               validation_data=single_step_window.val,
                               callbacks=[early_stopping],
                               verbose=2)
    print("✅ Linear model trained!")

    model_path = os.path.join(MODEL_DIR, "linear_model.keras")
    print(f"\nSaving linear model to {model_path}...")
    linear_model.save(model_path)
    print("✅ Linear model saved!")

    print("\n" + "="*60)
    print("Creating Golden Dataset for Evaluation")
    print("="*60)

    create_golden_dataset(
        test_df=test_df,
        window_config={
            'input_width': single_step_window.input_width,
            'label_width': single_step_window.label_width,
            'shift': single_step_window.shift,
            'label_column': 'T (degC)'
        },
        num_samples=args.num_samples, # Use the argument here
        output_path='data/linear_golden_dataset.json'
    )

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train a Linear Time Series model and create a golden dataset.")
    parser.add_argument(
        "--num_samples",
        type=int,
        default=50,
        help="Number of samples for the golden dataset. Use -1 for all possible samples."
    )
    args = parser.parse_args()
    main(args)



```


The general idea here is the same as the script: train_baseline_times_series.py.  The baseline has alot more lines of code, because I was validating my ideas of how it should work and correlating the outputs with the tutorial output.  
## Quick aside

WHY NO BASELINE CLASS IN the linear training script? AHA! You've correctly identified a key difference in the implementation patterns between the two scripts, and your
  question gets to the heart of how models can be defined in TensorFlow/Keras.

  
   * In train_baseline_time_series.py, we define a custom Python class: class Baseline(tf.keras.Model): ...
   * In train_linear_time_series.py, we don't have a class Linear(...). Instead, the model is created directly using: linear_model = tf.keras.Sequential([...])  (eg we are using tensorflow for its capabilities)

  The reason for this difference comes from the two primary ways to create models in Keras, and the tutorial uses both to teach these different styles.

  1. The tf.keras.Sequential API

  This is the simplest way to build a model in Keras. You use it when your model is a plain stack of layers, where each layer has exactly one input and one output.
  This will change later when we make more complex, 'sophisticated" (hahaha) models.
  The linear model is a perfect example of this. It's just a single Dense layer. Using Sequential is concise and easy to read for this kind of simple architecture.

  2. The tf.keras.Model Subclassing API

  This is the more powerful, flexible, and "Pythonic" way to build models. You use this when your model architecture is more complex. For example:
   * Models with multiple inputs or outputs.
   * Models with shared layers or non-linear connections (like residual connections).
   * Any model where you need full control over the forward pass.

  You subclass tf.keras.Model and define the forward pass logic yourself inside the call method.

  So, why was the Baseline model built using this more complex method?

  The Baseline model has very "unconventional" logic. Its job is to take the input and simply return a specific slice of it as the prediction. This isn't a standard
  neural network "layer." The clearest and most explicit way to define this custom behavior is to write the logic yourself inside a call method, which requires
  subclassing tf.keras.Model.

  In short:

   * We used tf.keras.Sequential for the linear model because it's a simple, standard stack of layers, and Sequential is the most direct way to represent that.
   * We used a custom class Baseline(tf.keras.Model) for the baseline model because its logic isn't a standard layer, and a custom call method was the clearest way to
     implement its unique behavior.

  The tutorial introduces both methods so that a practitioner learns the simple way (Sequential) for simple cases and the powerful way (subclassing) for everything
  else. The new linear_model is functionally identical to what it would be if we had wrapped it in a custom class; the Sequential approach is just less boilerplate
  code for this simple case.  TLDR: we did it the tutorial way, and we'll probably never have such a model ever again. Never, ever,ever will we ever have such a simple model.  ( i am absolutely going to eat those words.)



Differences:

```python
linear_model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, input_shape=(single_step_window.input_width, num_features))
    ])
# Save the model
linear.save('models/linear_model.keras')
print("✅ Linear model saved!")
```
Plus the aforementioned baseline class differene, of course.

**Run it:**

Run the new training script. Let's start by creating a small sample of 50 test cases for a quick run.
```bash
   python scripts/train_linear_time_series.py --num_samples 50
```
You can also run it with `--num_samples -1` to generate all 7008 test cases for a full evaluation.  Aha-this way allows you to check the whole test set, which will take alot longer, at least until you get the gpu working and run all these tests in parallel.  Put that underpowered laptop gpu to work and cook your cpu!

Anyways.


---

### Stage 2: Seed the Database

Create a seeding script (e.g., `seed_linear_test_cases.py`):
```python
# scripts/seed_linear_test_cases.py
import os
import json
import requests
import sys

# Add project root to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Configuration
API_URL = "http://localhost:8000/api/v1/prompts/"
GOLDEN_DATASET_PATH = "data/linear_golden_dataset.json"

def main():
    """
    Seeds the database with linear time series test cases from the golden dataset.
    """
    print(f"--- Seeding Linear Test Cases from Golden Dataset ---")
    
    # Load golden dataset
    if not os.path.exists(GOLDEN_DATASET_PATH):
        print(f"❌ Error: Golden dataset not found at {GOLDEN_DATASET_PATH}")
        print("Please run the training script first to generate it:")
        print("  python scripts/train_linear_time_series.py")
        return
    
    with open(GOLDEN_DATASET_PATH, 'r') as f:
        golden_cases = json.load(f)
    
    print(f"Loaded {len(golden_cases)} test cases from golden dataset")
    
    # Seed test cases
    total_created = 0
    failed = 0
    
    for case in golden_cases:
        case_id = case['case_id']
        metadata = case['metadata']
        
        # Create payload for API
        payload = {
            "test_case_name": f"Linear TS Prediction - Case {case_id}",
            "model_type": "time_series_linear",
            "input_type": "time_series_window",
            "output_type": "temperature_prediction",
            "input_data": case['input_data'],
            "ground_truth": case['ground_truth'],
            "category": "weather_forecasting",
            "tags": ["linear", "time_series", "temperature", "single_step"],
            "origin": "golden_dataset",
            "is_verified": True,
            "test_case_metadata": metadata
        }
        
        try:
            response = requests.post(
                API_URL, 
                headers={"Content-Type": "application/json"}, 
                data=json.dumps(payload)
            )
            
            if response.status_code == 200:
                total_created += 1
                if total_created % 50 == 0 and len(golden_cases) > 100: # Adjust printing for large sets
                    print(f"  ...created {total_created} test cases so far...")
            elif response.status_code == 409: # Conflict, test case likely exists
                print(f"  ⚠️  Test case {case_id} may already exist, skipping...")
                total_created +=1
            else:
                failed += 1
                print(f"  ❌ Failed to create case {case_id}. Status: {response.status_code}")
                if failed <= 3:
                    print(f"     Response: {response.text}")
                    
        except requests.exceptions.ConnectionError:
            print(f"\n❌ Error: Could not connect to API at {API_URL}")
            print("Please ensure your FastAPI application is running:")
            print("  uvicorn ml_eval.main:app --reload")
            return
        except Exception as e:
            failed += 1
            print(f"  ❌ Unexpected error for case {case_id}: {e}")
    
    print(f"\n{'='*60}")
    print(f"Seeding Complete!")
    print(f"  ✅ Handled: {total_created}/{len(golden_cases)} test cases")
    if failed > 0:
        print(f"  ❌ Failed: {failed}/{len(golden_cases)} test cases")
    print(f"{'='*60}")
    
    if total_created > 0:
        print("\nNext steps:")
        # Fix for the SyntaxError: unterminated string literal
        print("  1. Create a model run:\n     curl -X 'POST' 'http://localhost:8000/api/v1/runs/' \
       -H 'Content-Type: application/json' \
       -d '{\"model_name\": \"linear_model\", \"model_version\": \"1.0\", \"model_type\": \"time_series_linear\"}'")
        print("\n  2. Run evaluation:")
        print("     python scripts/run_evaluation.py <RUN_ID>")
        print("\n  3. Generate report:")
        print("     python scripts/gemerate_report_time_series.py <RUN_ID>")

if __name__ == "__main__":

```

**Run it:**
```bash
python scripts/seed_linear_test_cases.py
```
So we added in some error handling, and slighly different logic for the larger datset (ie the whole test set), but other than that , htis is functionally, grammatically and semantically the same a s the baseline seeding.  We will keep them both for examples, and to compare with more complex examples, probably.


---

### Stage 3: Register Model Type & Run Evaluation

**Edit `scripts/run_evaluation.py`** to add the new model type:
```python
# ... existing imports ...
from ml_eval.core.implementations.keras_time_series_model import KerasTimeSeriesModel
from ml_eval.core.adapters.keras_time_series_adapter import KerasTimeSeriesAdapter

# ... in main() function ...

# Existing baseline block
elif model_run.model_type == "baseline_time_series":
    baseline_model = BaselineTimeSeriesModel(model_path=model_path)
    model_adapter = BaselineTimeSeriesAdapter(model=baseline_model)
    evaluator = MeanSquaredErrorEvaluator()

# ADD THIS NEW BLOCK FOR LINEAR
elif model_run.model_type == "time_series_linear":
    model_path = os.path.join("models", f"{model_run.model_name}.keras")
    if not os.path.exists(model_path):
        print(f"❌ Error: Model file not found at {model_path}")
        return
    
    # Reuse the generic Keras wrapper (works because shapes match!)
    ts_model = KerasTimeSeriesModel(model_path=model_path)
    model_adapter = KerasTimeSeriesAdapter(model=ts_model)
    evaluator = MeanSquaredErrorEvaluator()  # Same evaluator!
```

**Why does this work?**

1. **KerasTimeSeriesModel** loads any `.keras` file - doesn't care if it's Linear, Dense, or anything else
2. **KerasTimeSeriesAdapter** just calls `.predict()` - works for any Keras model
3. **MeanSquaredErrorEvaluator** compares arrays - doesn't care what created them

**Create the model run:**
```bash
curl -X 'POST' 'http://localhost:8000/api/v1/runs/' \
  -H 'Content-Type: application/json' \
  -d '{
    "model_name": "linear_model",
    "model_version": "1.0",
    "model_type": "time_series_linear"
  }'
```

The line above should return you a bunch of data, including a RUN_ID.  Note that id.

**Run evaluation:**
```bash
python scripts/run_evaluation.py <RUN_ID>
```

---

### Stage 4: Generate Reports

**No changes needed!** The report generator works because:
- Linear outputs the same shape as baseline `(batch, 1, 1)`
- Uses the same metrics (MSE, MAE, RMSE)
- Evaluator stores results in the same database schema
```bash
python scripts/generate_report_time_series.py <RUN_ID>
```

**Outputs:**
- `linear_model_v<RUN_ID>_time_series_report.png`
- `linear_model_v<RUN_ID>_prediction_samples.png`
- `linear_model_v<RUN_ID>_tutorial_style.png`
- `linear_model_v<RUN_ID>_summary.csv`

---

## Comparing Results: Baseline vs Linear

Once you've run both models, compare their performance:
```bash
# View both reports side-by-side
ls reports/
baseline_model_v1_time_series_report.png
linear_model_v2_time_series_report.png
```

**Expected Results (from TensorFlow tutorial):**

| Model    | Validation MAE | Test MAE | Improvement |
|----------|----------------|----------|-------------|
| Baseline | 0.0785         | 0.0852   | (baseline)  |
| Linear   | ~0.070         | ~0.076   | ~10% better |

**Why is Linear better?**
- Baseline: Just repeats current temperature
- Linear: Learns weighted combination of all 19 features (pressure, humidity, wind, etc.)
- Linear can detect patterns like "when pressure drops, temperature often rises"

---

## Summary: When to Use This Approach

✅ **Use this simple approach when your model has:**
- Same input width as baseline (1 timestep)
- Same output width as baseline (1 timestep)
- Same prediction target (temperature)
- Standard Keras Sequential or Functional API

✅ **Models that fit this pattern:**
- Linear (single Dense layer)
- Dense (multiple Dense layers)
- Any feedforward network with matching shapes

❌ **Don't use this approach for:**
- CNN models (need `input_width=3`)
- RNN/LSTM models (need `input_width=24, label_width=24`)
- Multi-output models
- Models with custom Keras classes

For those cases, see: **"Onboarding a Complex Model: The CNN Example"**
(not yet implemented/writen yet)
---

## Key Takeaway

The magic of this approach is **shape compatibility**. Because Linear has identical input/output shapes to Baseline, we get **massive code reuse** - 95% of the infrastructure just works. The only custom code is the model definition itself!

This makes adding simple models incredibly fast and ensures consistency across your evaluation pipeline.

## UPDATE:  

I've created generate_report_time_series_v2.py with the improved report naming convention. (eg more identifying metadata)

  Here's an updated sequence of commands to run the linear model's evaluation using this new reporting script:

  Step 1: Train the Model & Create Golden Dataset

  Train your model and create the golden dataset. You can specify the number of samples using --num_samples.

   1 python scripts/train_linear_time_series.py --num_samples 50
  (Use `--num_samples -1` for all possible test cases).

  Step 2: Seed the Database

  Load the test cases into the database.

   1 python scripts/seed_linear_test_cases.py

  Step 3: Create the Evaluation Run

  Create the ModelRun entry, noting your new model_type as "time_series_linear".

   1 curl -X 'POST' 'http://localhost:8000/api/v1/runs/' \
   2   -H 'Content-Type: application/json' \
   3   -d '{"model_name": "linear_model", "model_version": "1.0", "model_type": "time_series_linear"}'
  This will output a run_id.

  Step 4: Run the Evaluation

  Execute the evaluation script with your <RUN_ID>.

   1 python scripts/run_evaluation.py <YOUR_RUN_ID>

  Step 5: Generate the Report (using the new script)

  Now, use the new report generation script, generate_report_time_series_v2.py, with your <RUN_ID>.

   1 python scripts/generate_report_time_series_v2.py <YOUR_RUN_ID>

  You will find the generated reports in the reports/ directory with detailed, self-describing filenames based on the model's name, version, configuration
  (if any), and the run ID.
